---
title: "Student alcohol consumption"
author: "Marek Fudaliński && Jakub Sroka"
date: "5/13/2020"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Dataset

dataset obtained from: https://www.kaggle.com/uciml/student-alcohol-consumption

### Context:
The data were obtained in a survey of students math courses in secondary school. It contains a lot of interesting social, gender and study information about students. You can use it for some EDA or try to predict students final grade.  

In ths notebook we will be trying to predict student alcohol consumption based on available data  
 
school - student's school (binary: 'GP' - Gabriel Pereira or 'MS' - Mousinho da Silveira)  
sex - student's sex (binary: 'F' - female or 'M' - male)  
age - student's age (numeric: from 15 to 22)  
address - student's home address type (binary: 'U' - urban or 'R' - rural)  
famsize - family size (binary: 'LE3' - less or equal to 3 or 'GT3' - greater than 3)  
Pstatus - parent's cohabitation status (binary: 'T' - living together or 'A' - apart)  
Medu - mother's education (numeric: 0 - none, 1 - primary education (4th grade), 2 – 5th to 9th grade, 3 – secondary education or 4 – higher education)  
Fedu - father's education (numeric: 0 - none, 1 - primary education (4th grade), 2 – 5th to 9th grade, 3 – secondary education or 4 – higher education)  
Mjob - mother's job (nominal: 'teacher', 'health' care related, civil 'services' (e.g. administrative or police), 'at_home' or 'other')  
Fjob - father's job (nominal: 'teacher', 'health' care related, civil 'services' (e.g. administrative or police), 'at_home' or 'other')  
reason - reason to choose this school (nominal: close to 'home', school 'reputation', 'course' preference or 'other')  
guardian - student's guardian (nominal: 'mother', 'father' or 'other')  
traveltime - home to school travel time (numeric: 1 - 1 hour)  
studytime - weekly study time (numeric: 1 - 10 hours)  
failures - number of past class failures (numeric: n if 1<=n<3, else 4)  
schoolsup - extra educational support (binary: yes or no)  
famsup - family educational support (binary: yes or no)  
paid - extra paid classes within the course subject (Math or Portuguese) (binary: yes or no)  
activities - extra-curricular activities (binary: yes or no)  
nursery - attended nursery school (binary: yes or no)  
higher - wants to take higher education (binary: yes or no)  
internet - Internet access at home (binary: yes or no)  
romantic - with a romantic relationship (binary: yes or no)  
famrel - quality of family relationships (numeric: from 1 - very bad to 5 - excellent)  
freetime - free time after school (numeric: from 1 - very low to 5 - very high)  
goout - going out with friends (numeric: from 1 - very low to 5 - very high)  
Dalc - workday alcohol consumption (numeric: from 1 - very low to 5 - very high)  
Walc - weekend alcohol consumption (numeric: from 1 - very low to 5 - very high)  
health - current health status (numeric: from 1 - very bad to 5 - very good)  
absences - number of school absences (numeric: from 0 to 93)  

These grades are related with the Math course:

G1 - first period grade (numeric: from 0 to 20)  
G2 - second period grade (numeric: from 0 to 20)  
G3 - final grade (numeric: from 0 to 20, output target)  

## First look at dataset

```{r mat-student}
matStudentds <- read.csv("data/student-mat.csv", header = TRUE)
head(matStudentds, 3)
```

```{r summary}
summary(matStudentds)
```

## Data Preparation

```{r data-preparation}
matStudentds$alc <- ceiling((as.numeric(matStudentds$Walc) + as.numeric(matStudentds$Dalc)) / 2)
matStudentds$alc
data <- subset(matStudentds, select = -c(Walc, Dalc))
head(data, 1)
```



### Linear regression to find alcohol consumption

```{r reg}
library(MASS)
attach(data)
lmFit <- lm(alc ~ ., data = data)
summary(lmFit)

```

Based on regresssion on all parametrs now we can check which parameters are significant and create a subset of the for further analysis. Just based on this test we can assume that the biggest risk of excesive alcohol consumptions exists within males that have bad relationship with their family, and often go out with their friends. This assumptions seems reasonable even without tests.

### Slimer model

```{r subset}

dataFiltered <-  subset(data, select =  c(sex, address, Fjob, reason, paid, activities, nursery, famrel, goout, health, absences, alc))
detach(data)
attach(dataFiltered)
head(dataFiltered, 1)
```

```{r fit}
lmSlimFit <- lm(alc ~ ., data = dataFiltered)
summary(lmSlimFit)
```

Since only paramater that left and is not category type field is no. absences we will check if there are some other than linear relations between this value and alc.

```{r absences} 
ab <- lm(alc ~ poly(absences, 3))
summary(ab)
```

We can observer significant relation up to second power, further results give high lvl of Pr(>|t|) and shouldn't be included.

```{r alc-absence}
absenc <- lm(alc ~ poly(absences, 2))
plot(absenc)
```

## Logistic prediction 

```{r pred log}
dataFilteredProblem <- dataFiltered
detach(dataFiltered)
dataFilteredProblem$alcProblem <- dataFiltered$alc > 2
dataFilteredProblem$alcProblem
dataFilteredProblem <- subset(dataFilteredProblem, select = -c(alc))

smp_size <- floor(0.8 * nrow(dataFilteredProblem))

set.seed(123)
train_ind <- sample(seq_len(nrow(dataFilteredProblem)), size = smp_size)

train <- dataFilteredProblem[train_ind, ]
test <- dataFilteredProblem[-train_ind, ]

head(train,2)
```

```{r fit log}
fit.logistic <- glm(train$alcProblem ~ ., data = train)
summary(fit.logistic)

probs.logistic <- predict(fit.logistic, type = "response")
head(probs.logistic)

pred.logistic <- ifelse(probs.logistic > 0.5, TRUE, FALSE)

mean(pred.logistic != train$alcProblem)
```

```{r fit log simple}
fit.logisticSimple <- glm(train$alcProblem ~ sex + goout, data = train)

summary(fit.logisticSimple)

probs.logisticSimple <- predict(fit.logisticSimple, type = "response")
head(probs.logisticSimple)

pred.logisticSimple <- ifelse(probs.logisticSimple > 0.5, TRUE, FALSE)

mean(pred.logisticSimple != train$alcProblem)
```


```{r test }
anova(fit.logisticSimple, fit.logistic)
```

```{r test2}
probs.logistic <- predict(fit.logistic, test, type = "response")
pred.logistic <- ifelse(probs.logistic > 0.5, TRUE, FALSE)
mean(pred.logistic != test$alcProblem)

probs.logisticSimple <- predict(fit.logisticSimple, test, type = "response")
pred.logisticSimple <- ifelse(probs.logisticSimple > 0.5, TRUE, FALSE)
mean(pred.logisticSimple != test$alcProblem)
```


### LDA

```{r lda}
fit.lda <- lda(train$alcProblem ~ ., data = train)
summary(fit.lda)

pred.lda <- predict(fit.lda, test)

table(pred.lda$class, test$alcProblem)
mean(pred.lda$class != test$alcProblem)
```

```{r qda}
fit.qda <- qda(train$alcProblem ~ ., data = train)
summary(fit.qda)

pred.qda <- predict(fit.qda, test)

table(pred.qda$class, test$alcProblem)
mean(pred.qda$class != test$alcProblem)
```


## Cross validation on bigger deegree simple model 


```{r cross validation}
library(boot)
max.degree <- 5

dataFilteredCV <- dataFilteredProblem
dataFilteredCV$sex= as.integer(as.factor(dataFilteredCV$sex))
dataFilteredCV$famrel= as.integer(as.factor(dataFilteredCV$famrel))
dataFilteredCV$alcProblem= as.integer(dataFilteredCV$alcProblem)
dataFilteredCV <- subset(dataFilteredCV, select = c(sex, famrel, alcProblem))
dataFilteredCV <- dataFilteredCV[1:390, ]
attach(dataFilteredCV)
comp.mse.cv <- function(degree, k) {
  fit.glm <- glm(alcProblem ~ poly(sex, famrel), data = dataFilteredCV)
  cv.glm(dataFilteredCV, fit.glm, K = k)$delta[1]
}

mse <- sapply(1:max.degree, comp.mse.cv, k = 10)
mse
detach(dataFilteredCV)
```

## Classification to specific group with KNN

```{r knn}

dataFilteredKNN <- dataFiltered
dataFilteredKNN$sex= as.integer(as.factor(dataFilteredKNN$sex))
dataFilteredKNN$address= as.integer(as.factor(dataFilteredKNN$address))
dataFilteredKNN$Fjob= as.integer(as.factor(dataFilteredKNN$Fjob))
dataFilteredKNN$reason= as.integer(as.factor(dataFilteredKNN$reason))
dataFilteredKNN$paid= as.integer(as.factor(dataFilteredKNN$paid))
dataFilteredKNN$activities= as.integer(as.factor(dataFilteredKNN$activities))
dataFilteredKNN$nursery= as.integer(as.factor(dataFilteredKNN$nursery))

smp_size <- floor(0.8 * nrow(dataFilteredKNN))

set.seed(123)
train_ind <- sample(seq_len(nrow(dataFilteredKNN)), size = smp_size)

train <- dataFilteredKNN[train_ind, ]
test <- dataFilteredKNN[-train_ind, ]

library(class)
pred.knn.1 <- knn(train, test, train$alc, k = 1)
table(pred.knn.1, test$alc)
mean(pred.knn.1 != test$alc)

#pred.knn.2 <- knn(train, test, train$alc, k = 2)
#table(pred.knn.2, test$alc)
#mean(pred.knn.2 != test$alc)

pred.knn.3 <- knn(train, test, train$alc, k = 3)
table(pred.knn.3, test$alc)
mean(pred.knn.3 != test$alc)

#pred.knn.4 <- knn(train, test, train$alc, k = 4)
#table(pred.knn.4, test$alc)
#mean(pred.knn.4 != test$alc)

#adding 5 7 (most common)

pred.knn.5 <- knn(train, test, train$alc, k = 5)
table(pred.knn.5, test$alc)
mean(pred.knn.5 != test$alc)

pred.knn.7 <- knn(train, test, train$alc, k = 7)
table(pred.knn.7, test$alc)
mean(pred.knn.7 != test$alc)
```




## Feature selection in non-linear models 
Using default regsubsets from leaps library so only max 12 features are sselected.

```{r}
library(leaps)
fit.bs <- regsubsets(alc ~ .,data = data,nvmax = 12)
fit.bs.summary <- summary(fit.bs)
fit.bs.summary$cp
bic.min <- which.min(fit.bs.summary$bic)
bic.min
fit.bs.summary$bic[bic.min]

plot(fit.bs.summary$bic, xlab = "Liczba zmiennych", ylab = "BIC", col = "green",
     type = "b", pch = 20)
points(bic.min, fit.bs.summary$bic[bic.min], col = "red", pch = 9)

plot(fit.bs, scale = "bic")

coef(fit.bs, id = 9)

#bic Schwartz's information criterion, BIC


```

As we can see in the above analysis the best choice is to take 9 features namely:
sexM, addressU, Fjobservices, studytime, paidyes, activitiesyes, famrel, goout, absences.


## Regularization GLMNET


Preparing variables
```{r }
library(glmnet)
X <- model.matrix(alc ~ ., data = data)[, -1]
y <- data$alc > 2
```



```{r }
print(head(data,3))
```

Features which we want to predict
```{r }
print(head(y,3))
```
```{r }
print(head(X,3))
```

(grzbietowa)
```{r }
lambda.grid <- 10^seq(10, -2, length.out = 100)
fit.ridge <- glmnet(X, y, alpha = 0, lambda = lambda.grid)
set.seed(1)
n <- nrow(X)
train <- sample(1:n, n / 2)
test <- -train
fit.ridge <- glmnet(X[train,], y[train], alpha = 0, lambda = lambda.grid,
                    thresh = 1e-12)
```


```{r }
set.seed(1)
cv.out <- cv.glmnet(X[train,], y[train], alpha = 0)
plot(cv.out)
cv.out$lambda.min
```

Minimal lambda
```{r }
pred.ridge <- predict(fit.ridge, s = cv.out$lambda.min, newx = X[test,])
mean((pred.ridge - y[test])^2)
```
Square error 0.17 in thic case means that only 60% of cases were properly classfied. (we need to make root to calculate accuracy)


Least squares method
```{r }
pred.ridge <- predict(fit.ridge, s = 0, newx = X[test,])
mean((pred.ridge - y[test])^2)
```

Somehow we have better results for λ=1
```{r }
pred.ridge <- predict(fit.ridge, s = 1, newx = X[test,])
mean((pred.ridge - y[test])^2)
```



```{r }
fit.ridge.full <- glmnet(X, y, alpha = 0)
predict(fit.ridge.full, s = cv.out$lambda.min, type = "coefficients")
```


```{r }
fit.ridge.full <- glmnet(X, y, alpha = 0)
predict(fit.ridge.full, s = 1, type = "coefficients")
```

#Lasso

```{r }
fit.lasso <- glmnet(X[train,], y[train], alpha = 1)
plot(fit.lasso, xvar = "lambda")
```


```{r }
cv.out <- cv.glmnet(X[train,], y[train], alpha = 1)
plot(cv.out)
cv.out$lambda.min
pred.lasso <- predict(fit.lasso, s = cv.out$lambda.min, newx = X[test,])
mean((pred.lasso - y[test])^2)
```
blad 0.18 means 43% failours in prediction

```{r }
fit.lasso.full <- glmnet(X, y, alpha = 1)
predict(fit.lasso.full, s = cv.out$lambda.min, type = "coefficients")[1:20,]
```

#Step function

step funcion for absences
```{r }
#X y
dataTmp <- data
attach(dataTmp)
dataTmp$alc <- dataTmp$alc > 2


absences.lims <- range(absences)
absences.grid <- seq(absences.lims[1], absences.lims[2])

#print(data$absences)
table(cut(absences, breaks = 6))

fit.step <- lm(alc ~ cut(absences, 6), data = dataTmp)
pred.step <- predict(fit.step, list(absences = absences.grid), se.fit = TRUE)
se.bands <- cbind(pred.step$fit + 2 * pred.step$se.fit, 
                  pred.step$fit - 2 * pred.step$se.fit)
plot(absences, alc, col = "darkgrey", cex = 0.5, xlim = absences.lims,ylim = c(-1,6))
lines(absences.grid, pred.step$fit, col = "red", lwd = 2)
matlines(absences.grid, se.bands, col = "red", lty = "dashed")

detach(dataTmp)


```
Steps based on absences, we can see that majority of students has less than 13 absences.


```{r }
attach(dataTmp)
age.lims <- range(age)
age.grid <- seq(age.lims[1], age.lims[2])

studytime.lims <- range(studytime)
studytime.grid <- seq(studytime.lims[1], studytime.lims[2])

detach(dataTmp)
```
step function for age

```{r }
#X y
attach(dataTmp)
age.lims <- range(age)
age.grid <- seq(age.lims[1], age.lims[2])

#print(data$age)
table(cut(age, breaks = 6))

fit.step <- lm(alc ~ cut(age, 6), data = dataTmp)
pred.step <- predict(fit.step, list(age = age.grid), se.fit = TRUE)
se.bands <- cbind(pred.step$fit + 2 * pred.step$se.fit, 
                  pred.step$fit - 2 * pred.step$se.fit)
plot(age, alc, col = "darkgrey", cex = 0.5, xlim = age.lims)
lines(age.grid, pred.step$fit, col = "red", lwd = 2)
matlines(age.grid, se.bands, col = "red", lty = "dashed")

#plot(age,alc)
detach(dataTmp)
```

##Natural splines

###Absences
```{r }
library(splines)
attach(data)
fit.ns <- lm(alc ~ ns(absences, df = 4), data = data)
pred.ns <- predict(fit.ns, list(absences = absences.grid), se.fit = TRUE)
plot(absences, alc, cex = 0.5, col = "darkgrey")
lines(absences.grid, pred.ns$fit, col = "red", lwd = 2)
lines(absences.grid, pred.ns$fit + 2 * pred.ns$se.fit, col = "red",
      lty = "dashed")
lines(absences.grid, pred.ns$fit - 2 * pred.ns$se.fit, col = "red",
      lty = "dashed")
abline(v = attr(ns(absences, df = 4), "knots"), lty = "dotted")
detach(data)
```

###Studytime
```{r }
library(splines)
attach(data)
fit.ns <- lm(alc ~ ns(studytime, df = 4), data = data)
pred.ns <- predict(fit.ns, list(studytime = studytime.grid), se.fit = TRUE)
plot(studytime, alc, cex = 0.5, col = "darkgrey")
lines(studytime.grid, pred.ns$fit, col = "red", lwd = 2)
lines(studytime.grid, pred.ns$fit + 2 * pred.ns$se.fit, col = "red",
      lty = "dashed")
lines(studytime.grid, pred.ns$fit - 2 * pred.ns$se.fit, col = "red",
      lty = "dashed")
abline(v = attr(ns(studytime, df = 4), "knots"), lty = "dotted")
detach(data)
```

##Decision Trees
###First we are going to classiffy simply high or low consumption 
```{r }
attach(data)

library(tree)
# <= 1 daje 25%
# <= 2 daje 17%
# <= 3 daje 7%

High <- factor(ifelse(alc <= 2, "No", "Yes"))
alcH <- data.frame(alc,High)

alc.high.tree <- tree(High ~ . - alc, data = data)
summary(alc.high.tree)

plot(alc.high.tree)
text(alc.high.tree, pretty = 0)

detach(data)

```
Classification error around 17.5%.
Mostly used features: "goout"      "absences"   "freetime"   "age"        "G2"         "G3"  



```{r }
alc.high.tree
```
Most important predicators are at the top of the tree and the ones which splits set into two subset of the closest size.



```{r }
attach(data)
set.seed(1)
n <- nrow(data)
train <- sample(1:n, 3*n / 4)
test <- -train
alc.high.tree <- tree(High ~ . - alc, data = data, subset = train)
tree.class <- predict(alc.high.tree, newdata = data[test,], type = "class")
table(tree.class, alcH$High[test])
mean(tree.class != alcH$High[test])
plot(alc.high.tree)
text(alc.high.tree, pretty = 0)
detach(data)
```
When we want to classify new data error is around 22%.

###pruning
```{r }
attach(data)
set.seed(1)
alc.high.cv <- cv.tree(alc.high.tree, FUN = prune.misclass)
alc.high.cv
plot(alc.high.cv$size, alc.high.cv$dev, type = "b")

size.opt <- alc.high.cv$size[which.min(alc.high.cv$dev)]
alc.high.pruned <- prune.misclass(alc.high.tree, best = size.opt)
plot(alc.high.pruned)
text(alc.high.pruned, pretty = 0)

pruned.class <- predict(alc.high.pruned, newdata = data[test,], 
                        type = "class")
table(pruned.class, alcH$High[test])
mean(pruned.class != alcH$High[test])
print(alc.high.pruned)
detach(data)
```

After pruning optimal tree has only 2 levels and most important nodes are: gout and G3(final grade). estimated error rate is equal to around 20% it is higher than i previous case.


## Random forests

### Bagging
```{r }
attach(dataTmp)
library(randomForest)
alc.bag <- randomForest(alc ~ ., data = dataTmp, mtry = 3, importance = TRUE)
alc.bag

plot(alc.bag, type = "l")

importance(alc.bag)
varImpPlot(alc.bag)
detach(dataTmp)
```
Lowest error is around 18% misclassyfication. There is no reason to have more than 150 trees.


# sprawdzenie ile zle sklasyfikowanych
```{r }
attach(dataTmp)
set.seed(1)
alc.bag <- randomForest(alc ~ ., data = dataTmp, subset = train, mtry = 3,
                         importance = TRUE,ntree = 250)
alc.pred.bag <- predict(alc.bag, newdata = dataTmp[test,])
mean((alc.pred.bag - dataTmp$alc[test])^2)


detach(dataTmp)
```
For 250 trees we can observe error rate around 16.5%.

```{r }
attach(dataTmp)
set.seed(2)
alc.bag.s <- randomForest(alc ~ ., data = dataTmp, subset = train, mtry = 3,
                         importance = TRUE, ntree = 70)
alc.pred.bag <- predict(alc.bag.s, newdata = dataTmp[test,])
mean((alc.pred.bag - dataTmp$alc[test])^2)

detach(dataTmp)
```
For 70 trees we can observe error rate around 17.4%.


```{r }
attach(dataTmp)
set.seed(2)
alc.bag.s <- randomForest(alc ~ ., data = dataTmp, subset = train, mtry = 3,
                         importance = TRUE, ntree = 10)
alc.pred.bag <- predict(alc.bag.s, newdata = dataTmp[test,])
mean((alc.pred.bag - dataTmp$alc[test])^2)

detach(dataTmp)
```
For 10 trees we can observe error rate around 18.6%.

## Summary
W powyższym projekcie udało nam się z zadawalającą dokładnością stworzyć klasyfikatory, których błąd wahał się pomiędzy 15-20 % złych odpowiedzi. Spodziewlaiśmy się że najlepsze wyniki uzyskamy dla klasyfikatorów nieliniowych, a w szczegolności lasów losowych, gdyż dane, których użyliśmy przeważnie dane kategoryczn, a nie ciągłe. Jednak duża częśc kategorii opierała się na podziale rozkładu ciągłego na kubełki np. relacja z rodziną od 1-5. Podejrzewamy że w dużym stopniu z tego powody, oraz z jakości samych danych dało to dobre wyniki predykcji z błędem na poziomie około 19 procent w przypadku regresji logistycznej oraz LDA. Regresjia liniowa była dobrym wstępem i pierwszym szacunkiem, który pozwolił nam wyłonić znaczące parametry do dalszych predykcji przy użyciu modeli liniowych . KNN i QDA okazały się być nieodpowiednimi modelami gdyż w ich przypadku błędy były dość znaczące(nawet 35 % błędu w przypadku KNN). Spliny i Lasso posłużyły nam głównie do obserwacji pewnych zależności i tendencji w używanych danych. 

